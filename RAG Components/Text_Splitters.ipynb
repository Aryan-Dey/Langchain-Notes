{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf51fa99",
   "metadata": {},
   "source": [
    "#### **Text Splitters**  \n",
    "Text Splitting is the process of breaking large text files (like PDFs, books, articles) into smaller, manageable pieces (called â€œchunksâ€) so that LLMs (like GPT) can process them effectively.\n",
    "\n",
    "**Why is it necessary?**  \n",
    "LLMs have context length limits, meaning they can only handle a limited number of tokens/words at once. If you give too much text, they either fail or give poor quality output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f0cf3",
   "metadata": {},
   "source": [
    "##### **Length Based Text Splitter**- \n",
    "\n",
    "Length-based splitting is the simplest and fastest method to break large text into smaller chunks by fixed size.  \n",
    "Large paragraph â†’ Split into 100-character chunks â†’ Each chunk becomes part of a list.  \n",
    "\n",
    "**Drawback:**   \n",
    "It doesnâ€™t consider grammar, word boundaries, or meaning.  \n",
    "May split in the middle of a word or sentence, leading to broken context and less meaningful embeddings.  \n",
    "\n",
    "For RAG-based applications:  \n",
    "Chunk Overlap = 10% to 20% of chunk size (e.g., if chunk size = 100 â†’ overlap = 10 to 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2e79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='CampusXDeepLearningCurriculum\\nA.ArtificialNeuralNetworkandhowtoimprovethem\\n1.BiologicalInspiration\\nâ— Understandingtheneuronstructureâ— Synapsesandsignaltransmissionâ— Howbiologicalconceptstranslatetoartificialneurons\\n2.HistoryofNeuralNetworks\\nâ— Earlymodels(Perceptron)â— BackpropagationandMLPsâ— The\"AIWinter\"andresurgenceofneuralnetworksâ— Emergenceofdeeplearning\\n3.PerceptronandMultilayerPerceptrons(MLP)\\nâ— Single-layerperceptronlimitationsâ— XORproblemandtheneedforhiddenlayersâ— MLParchitecture\\n4. LayersandTheirFunctions\\nâ— InputLayerâ—‹ Acceptinginputdataâ— HiddenLayersâ—‹ Featureextractionâ— OutputLayerâ—‹ Producingfinalpredictions\\n5.ActivationFunctions'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}, page_content='â— SigmoidFunctionâ—‹ Characteristicsandlimitationsâ— HyperbolicTangent(tanh)â—‹ Comparisonwithsigmoidâ— ReLU(RectifiedLinearUnit)â—‹ Advantagesinmitigatingvanishinggradientsâ— LeakyReLUandParametricReLUâ—‹ AddressingthedyingReLUproblemâ— SoftmaxFunctionâ—‹ Multi-classclassificationoutputs\\n6.ForwardPropagation\\nâ— Mathematicalcomputationsateachneuronâ— Passinginputsthroughthenetworktogenerateoutputs\\n7.LossFunctions\\nâ— MeanSquaredError(MSE)â—‹ Usedinregressiontasksâ— Cross-EntropyLossâ—‹ Usedinclassificationtasksâ— HingeLossâ—‹ UsedwithSVMsâ— Selectingappropriatelossfunctionsbasedontasks\\n8.Backpropagation\\nâ— Derivationusingthechainruleâ— Computinggradientsforeachlayerâ— Updatingweightsandbiasesâ— Understandingcomputationalgraphs\\n9.GradientDescentVariants\\nâ— BatchGradientDescentâ—‹ Prosandcons'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3'}, page_content=\"â— StochasticGradientDescent(SGD)â—‹ Advantagesinlargedatasetsâ— Mini-BatchGradientDescentâ—‹ BalancingbetweenbatchandSGD\\n10.OptimizationAlgorithms\\nâ— Momentumâ—‹ AcceleratingSGDâ— NesterovAcceleratedGradientâ—‹ Lookingaheadtothefuturepositionâ— AdaGradâ—‹ Adaptivelearningratesâ— RMSPropâ—‹ FixingAdaGrad'sdiminishinglearningratesâ— Adamâ—‹ CombiningmomentumandRMSProp\\n11.RegularizationTechniques\\nâ— L1andL2Regularizationâ—‹ Addingpenaltytermstothelossfunctionâ— Dropoutâ—‹ Preventingoverfittingbyrandomlydroppingneuronsâ— EarlyStoppingâ—‹ Haltingtrainingwhenvalidationlossincreases\\n12.HyperparameterTuning\\nâ— LearningRateâ—‹ Impactonconvergenceâ— BatchSizeâ—‹ Trade-offsbetweenspeedandstabilityâ— NumberofEpochsâ—‹ Avoidingoverfittingâ— NetworkArchitecture\"), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4'}, page_content='â—‹ Decidingdepthandwidthâ— Techniques:â—‹ Gridsearchâ—‹ RandomSearchâ—‹ Bayesianoptimization\\n13.VanishingandExplodingGradients\\nâ— Problemsindeepnetworksâ— Solutions:â—‹ Properweightinitializationâ—‹ UseofReLUactivationfunctions\\n14.WeightInitializationStrategies\\nâ— Xavier/GlorotInitializationâ— HeInitialization\\n15.BatchNormalization\\nâ— Normalizinginputsofeachlayerâ— Acceleratingtrainingâ— Reducingdependenceoninitialization\\nB.ConvolutionNeuralNetworks\\n1.ChallengeswithMLPsforImageData\\nâ— Highdimensionalityâ— Lackofspatialinvariance\\n2.AdvantagesofCNNs\\nâ— Parametersharingâ— Localconnectivity'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5'}, page_content='3.ConvolutionOperation\\nâ— UnderstandingKernels/Filtersâ—‹ Edgedetectionfiltersâ—‹ Featureextractionâ— MathematicalRepresentationâ—‹ Convolutionin2Dand3Dâ— Hyperparametersâ—‹ Kernelsize,depthâ— StrideandPaddingâ—‹ Controllingoutputdimensionsâ—‹ Typesofpadding:samevs.valid\\n4.ActivationFunctions\\nâ— ReLU(RectifiedLinearUnit)â—‹ Advantagesoversigmoid/tanhâ— Variantsâ—‹ LeakyReLUâ—‹ ELU(ExponentialLinearUnit)\\n5.PoolingLayers\\nâ— Purposeâ—‹ Dimensionalityreductionâ—‹ Translationinvarianceâ— TypesofPoolingâ—‹ Maxpoolingâ—‹ Averagepoolingâ— PoolingSizeandStride\\n6.FullyConnectedLayers\\nâ— TransitionfromConvolutionalLayersâ— Flatteningâ—‹ Converting2Dfeaturesto1D'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6'}, page_content='7.LossFunctions\\nâ— Cross-EntropyLossforClassificationâ— MeanSquaredErrorforRegression\\n8.CNNArchitecture\\nLayerStacking\\nâ— Convolutional ->Activation->Pooling\\nFeatureMaps\\nâ— Understandingdepthandchannels\\nVisualization\\nâ— Interpretinglearnedfeatures\\n9. DataPreprocessingTechniques-DataNormalization\\nâ— ScalingPixelValuesâ—‹ 0-1normalizationâ—‹ Standardization(z-score)\\n10.DataPreprocessingTechniques-DataAugmentation\\nâ— Techniquesâ—‹ Rotation,flipping,croppingâ—‹ Colorjitter,noiseadditionâ— Purposeâ—‹ Reducingoverfittingâ—‹ Increasingdatasetdiversity\\nCNNArchitecturesandInnovations\\n11.LeNet-5\\nâ— ArchitectureDetails'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7'}, page_content='â—‹ Layers,activationsâ— Contributionsâ—‹ Handwrittendigitrecognition\\n12.AlexNet\\nâ— Breakthroughsâ—‹ Deepernetworkâ—‹ UseofReLUâ— ImpactonImageNetChallenge\\n13.VGGNetworks\\nâ— VGG-16andVGG-19â— DesignPhilosophyâ—‹ Usingsmallfilters(3x3)â—‹ Deepbutuniformarchitecture\\n14.InceptionNetworks(GoogLeNet)\\nâ— InceptionModulesâ—‹ Parallelconvolutionallayersâ— Motivationâ—‹ Efficientcomputation\\n15.ResNet(ResidualNetworks)\\nâ— ResidualBlocksâ—‹ Identitymappingsâ—‹ Shortcutconnectionsâ— SolvingVanishingGradientProblemâ— Variantsâ—‹ ResNet-50,ResNet-101\\n16.MobileNets\\nâ— DepthwiseSeparableConvolutions'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8'}, page_content='â— OptimizationsforMobileDevices\\n17.Pre-trainedModels&TransferLearning\\nâ— UsingModelsTrainedonImageNetâ— Fine-Tuningvs.FeatureExtraction\\nObjectDetectionandLocalization\\n18.TraditionalMethods\\nâ— SlidingWindowApproach\\n19.ModernArchitecture\\nâ— Region-BasedCNNs(R-CNN)â—‹ R-CNNâ—‹ FastR-CNNâ—‹ FasterR-CNNâ— YouOnlyLookOnce(YOLO)â— SingleShotMultiBoxDetector(SSD)â— MaskR-CNNâ—‹ Instancesegmentation\\nSemanticSegmentation\\n20.FullyConvolutionalNetworks(FCN)\\nâ— ReplacingFullyConnectedLayers\\n21.U-Net\\nâ— Encoder-DecoderArchitectureâ— SkipConnections\\nGenerativeModelswithCNNs\\n22.Autoencoders'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9'}, page_content='â— ConvolutionalAutoencodersâ—‹ Imagereconstructionâ— VariationalAutoencoders(VAE)\\n23.GenerativeAdversarialNetworks(GANs)\\nâ— DCGANâ—‹ UsingCNNsinGANsâ— Applicationsâ—‹ Imagegenerationâ—‹ Super-resolution\\nC.RecurrentNeuralNetworks\\n1.ArchitectureofRNNs\\nâ— SequentialDataChallengesâ— BasicRNNStructureâ— MathematicalFormulationâ— ActivationFunctions\\n2.ForwardPropagationThroughTime\\nâ— SequenceInputProcessingâ—‹ Handlingvariable-lengthsequencesâ— OutputGenerationâ—‹ Ateachtimesteporaftertheentiresequence\\n3.BackpropagationThroughTime(BPTT)\\nâ— UnfoldingtheRNNâ—‹ TreatingRNNasadeepnetworkovertimeâ— CalculatingGradientsâ—‹ Applyingthechainrulethroughtimestepsâ— ComputationalComplexityâ—‹ Memoryandcomputationconsiderations'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10'}, page_content='4.ChallengesinTrainingRNNs\\nâ— VanishingGradientsâ—‹ Gradientsdiminishoverlongsequencesâ— ExplodingGradientsâ—‹ Gradientsgrowexponentiallyâ— Solutionsâ—‹ Gradientclippingâ—‹ Advancedarchitectures(e.g.,LSTMs,GRUs)\\n5.LSTM\\nâ— LSTMcorecomponentsâ— GatesinLSTMâ— IntuitionBehindLSTMsâ— BackpropagationThroughTime\\n6.GRU\\nâ—‹ GRUcorecomponentsâ—‹ GatesinGRUâ—‹ IntuitionBehindGRUâ—‹ BackpropagationinGRUsâ—‹ GRUvsLSTM\\n6.DeepRNNs\\nâ—‹ StackingRNNlayersâ—‹ VanishingandExplodingGradientsinDeepRNNsâ—‹ UsingLSTMandGRUâ—‹ SolutionandtechniquestoovercomeVGPandEGPâ—‹ ResidualConnectionsâ—‹ Regularization\\n7.BidirectionalRNNs\\nâ—‹ MotivationbehindBidirectionalRNNs'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11'}, page_content='â—‹ BidirectionalRNNarchitectureâ—‹ ForwardandBackwardpassâ—‹ Combiningoutputsâ—‹ BidirectionalLSTM\\n8.ApplicationsofRNNs\\nâ—‹ Languagemodeling-Nextwordpredictionâ—‹ SentimentAnalysisâ—‹ POSTaggingâ—‹ Timeseriesforecasting\\nSeq2SeqNetworks\\n1.Encoder-DecoderNetworks\\nA.IntroductiontoEncoder-DecoderArchitecture\\nâ— PurposeandMotivationâ—‹ Handlingvariable-lengthinputandoutputsequences.â—‹ Essentialfortaskslikemachinetranslation,textsummarization,andspeechrecognition.\\nB.ComponentsofEncoder-DecoderNetworks\\nâ— Encoderâ—‹ Processestheinputsequenceandencodesitintoafixed-lengthcontextvector.â—‹ Architecture:TypicallyusesRecurrentNeuralNetworks(RNNs),LongShort-TermMemory(LSTM),orGatedRecurrentUnits(GRUs).'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12'}, page_content='â— Decoderâ—‹ Generatestheoutputsequencefromthecontextvector.â—‹ Architecture:Similartotheencoderbutfocusesonproducingoutputs.\\nC.MathematicalFormulation\\nâ— EncoderandDecoder Equations\\nD.ImplementationDetails\\nâ— HandlingVariable-LengthSequencesâ—‹ Padding:Addingzerostosequencestoensureuniformlength.â—‹ Masking:Ignoringpaddedelementsduringcomputation.â— LossFunctionsâ—‹ Cross-EntropyLoss:Commonlyusedforclassificationtasksateachtimestep.â— TrainingTechniquesâ—‹ TeacherForcing:Usingtheactualoutputasthenextinputduringtrainingtospeedupconvergence.\\nE.LimitationsofBasicEncoder-DecoderModels\\nâ— Fixed-LengthContextVectorBottleneckâ—‹ Difficultyincapturingallnecessaryinformationfromlonginputsequences.â— SolutionOverviewâ—‹ Introductionofattentionmechanismstoallowthemodeltofocusonrelevantpartsoftheinputsequence.\\n2.AttentionMechanismsandTheirTypes\\nA.MotivationforAttention'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13'}, page_content='â— OvercomingtheBottleneckâ—‹ Attentionallowsthemodeltoaccessallencoderhiddenstatesratherthancompressingallinformationintoasinglecontextvector.â— Benefitsâ—‹ Improvedperformanceonlongsequences.â—‹ Enhancedabilitytocapturealignmentbetweeninputandoutputsequences.\\nB.TypesofAttentionMechanisms\\n1.AdditiveAttention(BahdanauAttention)\\nâ— Conceptâ—‹ Calculatesalignmentscoresusingafeedforwardnetworkâ— Characteristicsâ—‹ Consideredmorecomputationallyintensiveduetoadditionalparameters.\\n2.MultiplicativeAttention(LuongAttention)\\nâ— Conceptâ—‹ Calculatesalignmentscoresusingdotproducts.â—‹ ScaledDotProduct:Adjustsfordimensionality.â— Characteristicsâ—‹ Moreefficientthanadditiveattention.\\nC.AttentionMechanismSteps\\n1. CalculateAlignmentScores2. ComputeAttentionWeights3. ComputeContextVector4. UpdateDecoderState\\nD.ImplementingAttentioninSeq2SeqModels\\nâ— IntegrationwithDecoder'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14'}, page_content='â—‹ Modifythedecodertoincorporatethecontextvectorateachtimestep.â— TrainingAdjustmentsâ—‹ Backpropagatethroughtheattentionmechanism.\\nE.VisualizationandInterpretation\\nâ— AttentionWeightsMatrixâ—‹ Visualizingwhichinputtokensthemodelattendstoduringeachoutputgenerationstep.â— Applicationsâ—‹ Erroranalysis.â—‹ Modelinterpretability.\\n3.TransformerArchitectures\\nA.LimitationsofRNN-BasedSeq2SeqModels\\nâ— SequentialProcessingâ—‹ RNNsprocessinputssequentially,hinderingparallelization.â— Long-TermDependenciesâ—‹ Difficultyincapturingrelationshipsbetweendistanttokens.\\nB.IntroductiontoTransformers\\nâ— KeyInnovationsâ—‹ Self-AttentionMechanism:Allowsthemodeltorelatedifferentpositionsofasinglesequencetocomputerepresentations.â—‹ PositionalEncoding:Injectsinformationaboutthepositionofthetokensinthesequence.â— Advantagesâ—‹ Improvedparallelization.â—‹ Betteratcapturingglobaldependencies.'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15'}, page_content='C.ComponentsofTransformerArchitecture\\n1.Multi-HeadSelf-Attention\\nâ— Conceptâ—‹ Multipleattentionmechanisms(heads)operatinginparallel.â— Processâ—‹ Query(Q),Key(K),andValue(V)matricesarecomputedfrominputembeddings.â—‹ Theattentionmechanismcalculatesaweightedsumofthevalues,withweightsderivedfromthequeriesandkeys.\\n2.PositionalEncoding\\nâ— Purposeâ—‹ Sincetransformersdonothaverecurrenceorconvolution,positionalencodingprovidesthemodelwithinformationaboutthepositionofeachtoken.â— Techniquesâ—‹ SinusoidalFunctions:â—‹ LearnedEmbeddings\\n3.FeedforwardNetworks\\nâ— Architectureâ—‹ Position-wisefullyconnectedlayersappliedindependentlytoeachposition.â— ActivationFunctionsâ—‹ TypicallyReLUorGELU.\\n4.LayerNormalization\\nâ— Purposeâ—‹ Normalizesinputsacrossthefeaturestostabilizeandacceleratetraining.\\n5.ResidualConnections'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16'}, page_content='â— Purposeâ—‹ Helpsintrainingdeepernetworksbymitigatingthevanishinggradientproblem.â— Implementationâ—‹ Addingtheinputofalayertoitsoutputbeforeapplyingtheactivationfunction.\\nD.TransformerEncoder-DecoderStructure\\nâ— EncoderStackâ—‹ Composedofmultipleidenticallayers,eachcontaining:â–  Multi-headself-attentionlayer.â–  Feedforwardnetwork.â— DecoderStackâ—‹ Similartotheencoderbutincludes:â–  Maskedmulti-headself-attentionlayertopreventpositionsfromattendingtosubsequentpositions.â–  Encoder-decoderattentionlayer.\\nE.ImplementingTransformers\\nâ— KeyStepsâ—‹ EmbeddingLayer:Convertsinputtokensintodensevectors.â—‹ AddingPositionalEncoding:Combinespositionalinformationwithembeddings.â—‹ BuildingEncoderandDecoderLayers:Stackmultiplelayersasperthearchitecture.â—‹ OutputLayer:Generatesfinalpredictions,oftenfollowedbyasoftmaxfunction.\\n4.TypesofTransformers\\nA.BERT(BidirectionalEncoderRepresentationsfromTransformers)'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17'}, page_content='â— Purposeâ—‹ Pre-trainingdeepbidirectionalrepresentationsbyjointlyconditioningonbothleftandrightcontext.â— Architectureâ—‹ Usesonlytheencoderpartofthetransformer.â— Pre-TrainingObjectivesâ—‹ MaskedLanguageModeling(MLM):Predictingmaskedtokensintheinput.â—‹ NextSentencePrediction(NSP):Predictingiftwosentencesfolloweachother.\\nB.GPT(GenerativePre-trainedTransformer)\\nâ— Purposeâ—‹ Focusedonlanguagegenerationtasks.â— Architectureâ—‹ Usesonlythedecoderpartofthetransformerwithmaskedself-attentiontopreventinformationflowfromfuturetokens.â— TrainingObjectiveâ—‹ CausalLanguageModeling(CLM):Predictingthenextwordinasequence.\\nC.OtherNotableTransformers\\nâ— RoBERTaâ—‹ ImprovesonBERTbytrainingwithlargerbatchesandmoredata.â— ALBERTâ—‹ Reducesmodelsizebysharingparametersandfactorizingembeddings.â— T5(Text-to-TextTransferTransformer)â—‹ TreatseveryNLPtaskasatext-to-textproblem.\\n5.Fine-TuningTransformers'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18'}, page_content='A.ConceptofFine-Tuning\\nâ— TransferLearningâ—‹ Adaptingapre-trainedmodeltoadownstreamtaskwithtask-specificdata.\\nB.StepsinFine-Tuning\\n1. LoadingPre-TrainedModelâ—‹ Usepre-trainedweightsfrommodelslikeBERT,GPT,etc.2. ModifyingOutputLayersâ—‹ Replacethefinallayertosuitthespecifictask(e.g.,classificationhead).3. AdjustingHyperparametersâ—‹ Learningrate,batchsize,numberofepochs.4. TrainingonTask-SpecificDataâ—‹ Uselabeleddatarelevanttothetask.\\nC.BestPractices\\nâ— Layer-WiseLearningRatesâ—‹ Applydifferentlearningratestodifferentlayers.â— AvoidingCatastrophicForgettingâ—‹ Usesmallerlearningratestopreventthemodelfromlosingpre-trainedknowledge.â— RegularizationTechniquesâ—‹ Dropout,weightdecay.\\nD.CommonFine-TuningTasks\\nâ— TextClassificationâ— NamedEntityRecognitionâ— QuestionAnsweringâ— TextSummarization'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19'}, page_content='6.Pre-TrainingTransformers\\nA.Pre-TrainingObjectives\\nâ— MaskedLanguageModeling(MLM)â—‹ Predictingmaskedtokensintheinputsequence.â— CausalLanguageModeling(CLM)â—‹ Predictingthenexttokengiventheprevioustokens.â— Sequence-to-SequencePre-Trainingâ—‹ UsedinmodelslikeT5.\\nB.DataPreparation\\nâ— CorpusSelectionâ—‹ Largeanddiversedatasets(e.g.,Wikipedia,CommonCrawl).â— TokenizationStrategiesâ—‹ WordPiece:UsedbyBERT.â—‹ Byte-PairEncoding(BPE):UsedbyGPT.\\nC.TrainingStrategies\\nâ— DistributedTrainingâ—‹ UsingmultipleGPUsorTPUs.â— MixedPrecisionTrainingâ—‹ Reducesmemoryusageandincreasesspeed.â— OptimizationAlgorithmsâ—‹ Adamoptimizerwithweightdecay(AdamW).\\nD.ChallengesinPre-Training\\nâ— ComputeResourcesâ—‹ Requiressignificantcomputationalpower.â— DataQualityâ—‹ Noisydatacanaffectmodelperformance.'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20'}, page_content='E.EvaluationofPre-TrainedModels\\nâ— Benchmarkingâ—‹ UsingdatasetslikeGLUE,SQuADtoassessperformance.â— AblationStudiesâ—‹ Understandingtheimpactofdifferentcomponents.\\n7.OptimizingTransformers\\nA.ComputationalChallenges\\nâ— HighMemoryConsumptionâ—‹ Duetoself-attentionmechanisms.â— LongTrainingTimes\\nB.OptimizationTechniques\\n1.EfficientAttentionMechanisms\\nâ— SparseAttentionâ—‹ Reducesthenumberofcomputationsbyfocusingonlocalpatterns.â— LinearizedAttention(Linformer)â—‹ Approximatesattentiontoreducecomplexity.â— Reformerâ—‹ Useslocality-sensitivehashingtoreducecomplexity.\\n2.ModelCompression\\nâ— Quantizationâ—‹ Reducingtheprecisionofweights(e.g.,from32-bitto8-bit).â— Pruningâ—‹ Removinglessimportantweightsorneurons.â— KnowledgeDistillation'), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21'}, page_content=\"â—‹ Trainingasmallermodel(student)toreplicatethebehaviorofalargermodel(teacher).\\nC.HardwareConsiderations\\nâ— GPUsvs.TPUsâ—‹ TPUscanofferfastercomputationfortensoroperations.â— ParallelismStrategiesâ—‹ DataParallelismâ–  Distributingdataacrossmultipledevices.â—‹ ModelParallelismâ–  Distributingthemodel'slayersacrossdevices.\\nD.SoftwareTools\\nâ— OptimizedLibrariesâ—‹ HuggingFaceTransformers:Providesoptimizedimplementations.â—‹ DeepSpeed:Optimizesmemoryandcomputation.â—‹ NVIDIAApex:Enablesmixedprecisiontraining.\\n8.NLPApplicationsUsingTransformers\\nA.TextClassification\\nâ— SentimentAnalysisâ—‹ Classifyingtextaspositive,negative,orneutral.â— TopicClassificationâ—‹ Categorizingtextintopredefinedtopics.\\nB.QuestionAnswering\\nâ— ImplementingQASystemsâ—‹ UsingmodelslikeBERTtofindanswerswithinacontext.\"), Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Deep Learning Curriculum', 'source': 'Data Sources\\\\dl-curriculum.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22'}, page_content='â— Datasetsâ—‹ SQuAD,TriviaQA.\\nC.MachineTranslation\\nâ— TransformerModelsâ—‹ ImplementingtranslationsystemswithoutRNNs.â— Datasetsâ—‹ WMTdatasets.\\nD.TextSummarization\\nâ— AbstractiveSummarizationâ—‹ GeneratingconcisesummariesusingmodelslikeT5.â— Datasetsâ—‹ CNN/DailyMail,Gigaword.\\nE.LanguageGeneration\\nâ— Chatbotsâ—‹ CreatingconversationalagentsusingGPTmodels.â— StoryGenerationâ—‹ Generatingcoherentnarratives.\\nF.NamedEntityRecognition\\nâ— SequenceLabelingâ—‹ Identifyingentitieslikenames,locations,dates.â— Fine-Tuningâ—‹ Adaptingpre-trainedmodelsforNERtasks.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path='Data Sources\\\\dl-curriculum.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separator=\"\")\n",
    "\n",
    "result = splitter.split_documents(docs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e665f5",
   "metadata": {},
   "source": [
    "##### **Text Structure Based Splitting - RecursiveCharacterTextSplitting**  \n",
    "This technique breaks large text into smaller chunks using the natural structure of the text: Paragraphs (\\n\\n), Sentences (\\n), Words ( ), Characters.  \n",
    "It tries to preserve meaning by avoiding mid-word or mid-sentence cuts, unlike fixed-length (length-based) splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4b9f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We', 'words. We can leverage this inherent structure to inform our splitting strategy, creating split', 'split that maintain natural language flow, maintain semantic coherence within split, and adapts to', 'adapts to varying levels of text granularity. LangChainâ€™s RecursiveCharacterTextSplitter implements', 'this concept:', 'The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.', 'If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).', 'This process continues down to the word level if necessary.']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\" Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChainâ€™s RecursiveCharacterTextSplitter implements this concept:\n",
    "The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n",
    "If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
    "This process continues down to the word level if necessary.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908fe6d",
   "metadata": {},
   "source": [
    "##### **Document Structure Based Text Splitting**  \n",
    "Unlike normal text (which is split using paragraphs, sentences, and words), some documents like code, markdown, or HTML are not structured in natural language. So they need to be split based on their document-specific constructs.  \n",
    "\n",
    "Use **RecursiveCharacterTextSplitter.from_language()**- LangChain allows you to split these document types using a custom separator list for each language.  \n",
    "You can apply it to: Python code, Java, JS, HTML, Markdown files, Other structured formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e6b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['class Student:\\n    def __init__(self, name, age, grade):\\n        self.name = name\\n        self.age = age\\n        self.grade = grade  # Grade is a float (like 8.5 or 9.2)\\n\\n    def get_details(self):\\n        return self.name\"\\n\\n    def is_passing(self):\\n        return self.grade >= 6.0', '# Example usage\\nstudent1 = Student(\"Aarav\", 20, 8.2)\\nprint(student1.get_details())\\n\\nif student1.is_passing():\\n    print(\"The student is passing.\")\\nelse:\\n    print(\"The student is not passing.\")']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "text = \"\"\"\n",
    "class Student:\n",
    "    def __init__(self, name, age, grade):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.grade = grade  # Grade is a float (like 8.5 or 9.2)\n",
    "\n",
    "    def get_details(self):\n",
    "        return self.name\"\n",
    "\n",
    "    def is_passing(self):\n",
    "        return self.grade >= 6.0\n",
    "\n",
    "\n",
    "# Example usage\n",
    "student1 = Student(\"Aarav\", 20, 8.2)\n",
    "print(student1.get_details())\n",
    "\n",
    "if student1.is_passing():\n",
    "    print(\"The student is passing.\")\n",
    "else:\n",
    "    print(\"The student is not passing.\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(len(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b604a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "# Project Name: Smart Student Tracker\n",
      "\n",
      "A simple Python-based project to manage and track student data, including their grades, age, and academic status.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "# Project Name: Smart Student Tracker\n",
    "\n",
    "A simple Python-based project to manage and track student data, including their grades, age, and academic status.\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "- Add new students with relevant info\n",
    "- View student details\n",
    "- Check if a student is passing\n",
    "- Easily extendable class-based design\n",
    "\n",
    "\n",
    "## ðŸ›  Tech Stack\n",
    "\n",
    "- Python 3.10+\n",
    "- No external dependencies\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Clone the repo  \n",
    "   ```bash\n",
    "   git clone https://github.com/your-username/student-tracker.git\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the splitter\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# Perform the split\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af462e7",
   "metadata": {},
   "source": [
    "##### **Semantic Meaning Based Text Splitting**  \n",
    "technique splits text based on meaning, not on length or structure.  \n",
    "\n",
    "**Why Semantic Splitting is Needed**  \n",
    "It detects shifts in topic by comparing sentence meanings.  \n",
    "It avoids mixing unrelated content in a single chunk.  \n",
    "Useful in cases where contextual clarity is crucial for downstream tasks (like embeddings or RAG).\n",
    "\n",
    "**Performance not very accurate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08fcc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['\\nFarmers were working hard in the fields, preparing the soil and planting seeds for the next season.', 'The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams. Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.', '']\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "text = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass.\n",
    "The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=0.1,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(len(chunks))\n",
    "print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
